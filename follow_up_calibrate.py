from math import *
import cv2.aruco as aruco
from utils.tranformUtils import *
from utils.depthUtils import *
import os
import datetime

root_path = os.getcwd() + '/'

def calibrate(mtx, dist, L, theta_x, side_length, seperation, marker_X, marker_Y, img_abspath, coor_abspath):
    '''
    :param mtx: 内参
    :param dist: 畸变参数
    :param L: 传送带从拍照位置到走点位置移动的距离
    :param theta_x: 传送带移动方向（x轴正方向）与机器人坐标系x轴正方向的夹角，单位：度(°)
    :param side_length: marker板tag的边长
    :param seperation: marker板tag之间的间距
    :param marker_X: 行方向上的标记个数
    :param marker_Y: 列方向上的标记个数
    :param img_abspath: 图片的绝对路径
    :param coor_abspath: 坐标文件的绝对路径
    :return: 手眼矩阵结果
    '''

    # 将角度转化为rad
    theta_x = theta_x * pi / 180
    theta_y = (theta_x + 90) * pi / 180

    # 读取机器人走点的数据
    data_pos1 = []
    with open(coor_abspath) as rf:
        coors = rf.readlines()
        tmp = {}
        points_num = int(len(coors))
        for i in range(points_num):
            coors[i] = coors[i].split()
            tmp['x'] = float(coors[i][0]) / 1000
            tmp['y'] = float(coors[i][1]) / 1000
            tmp['z'] = float(coors[i][2]) / 1000
            data_pos1.append(tmp)
            tmp = {}

    # 2590 * 1942
    img_data = cv2.imread(img_abspath, -1)
    img_data = cv2.undistort(img_data, mtx, dist)
    if len(img_data.shape) == 3:
        # 通道为3，则转为灰度图
        img_data = cv2.cvtColor(img_data, cv2.COLOR_BGR2GRAY)
    else:
        # 通道已经为1，则不变
        pass
    dictionary = aruco.Dictionary_get(aruco.DICT_6X6_100)
    board = aruco.GridBoard_create(marker_X, marker_Y, side_length, seperation, dictionary, 0)
    parameters = aruco.DetectorParameters_create()
    # corners是List， ids是narray
    corners, ids, rejectedImgPoints = aruco.detectMarkers(img_data, dictionary, parameters=parameters)

    # 画出检测边缘
    # aruco.drawDetectedMarkers(img_data, corners, ids, (0, 255, 0))
    # cv2.namedWindow("~", cv2.WINDOW_NORMAL)
    # cv2.imshow("~", img_data)
    # cv2.waitKey(0)

    assert len(ids) == points_num, "tag没有完全识别！请重新拍照"

    # 将id和对应的角点拼接在一起，并且按照id大小依次排序，最终得到排序后的角点
    # 仅适用于自己进行PNP对准，用官方的estimatePoseBoard不用进行此操作
    # corners = np.array(corners)
    # concat = np.zeros([points_num, 1, 1, 2])
    # for num in range(points_num):
    #     concat[num, 0, 0, 0] = ids[num, 0]
    # concat = np.append(corners, concat, axis=2)
    # concat = list(concat)
    # concat.sort(key=lambda num: int(num[0][4][0]))
    # corners = list(np.array(concat, np.float32)[:, :, :4, :])
    # ids = np.array(concat)[:, :, 4, 0]

    # get the size of image
    img_size = (list(img_data.shape)[1], list(img_data.shape)[0])

    # a是角点在机器人坐标系下的坐标
    # b是角点的像素坐标
    a = np.ndarray([points_num, 1, 3], dtype=np.float32)
    b = np.ndarray([points_num, 1, 2], dtype=np.float32)
    c = np.ndarray([points_num, 2], dtype=np.float32)
    rob_points_list = []
    img_points_list = []
    sum_z_on_the_belt = 0

    if len(corners) == points_num:
        for i in range(points_num):
            # 基准点在机器人底座坐标系下的坐标
            # 以传送带移动方向为正方向，沿其反方向延伸，所以是-L
            datum_coor_x = data_pos1[i]['x'] + cos(theta_x) * -L
            datum_coor_y = data_pos1[i]['y'] + cos(theta_y) * -L
            datum_coor_z = data_pos1[i]['z']
            a[i, 0, :] = np.array([datum_coor_x, datum_coor_y, datum_coor_z])
            # the shape of corners[i] is (1 ,4, 2)
            # if the points generated by robots change, then change '*' within corners[i][0][*, :]
            b[i, 0, :] = corners[i][0][0, :]
            c[i, :] = corners[i][0][0, :]
            cv2.putText(img_data, str(int(ids[i][0])), (int(b[i, 0, 0]), int(b[i, 0, 1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)
            cv2.circle(img_data, (int(b[i, 0, 0]), int(b[i, 0, 1])), 5, (0, 0, 255), -1)

        rob_points_list.append(a)
        img_points_list.append(b)
        cv2.namedWindow("corner_points", cv2.WINDOW_NORMAL)
        cv2.imshow("corner_points", img_data)
        cv2.waitKey(1000)

    # transform 板->相机
    # rvecs, tvecs, _ = aruco.estimatePoseSingleMarkers(corners, side_length, mtx, dist)
    est_num, rvec, tvec = aruco.estimatePoseBoard(corners, ids, board, mtx, dist, None, None)

    # 画出坐标轴
    aruco.drawAxis(img_data, mtx, dist, rvec, tvec, 0.4)
    cv2.namedWindow("~", cv2.WINDOW_NORMAL)
    cv2.imshow("~", img_data)
    cv2.waitKey(1000)

    # ones = np.ones([1, points_num])
    # img_points_homo = np.append(c.T, ones, axis=0)
    # cam_points = np.dot(np.linalg.inv(mtx), img_points_homo)
    # ret, mtx, dist, rvec, tvec = cv2.calibrateCamera([mpoints_self_mtx], [c], img_size, None, None)
    # ret, mtx, dist, rvec, tvec = cv2.calibrateCamera([mpoints_in_self], img_points_list, img_size, mtx, dist)

    # obtain the extrinsic (the rotation matrix "R" and the translation vector "tvec")
    R, _ = cv2.Rodrigues(rvec)
    extrinsic = np.append(np.append(R, tvec, axis=1), np.array([[0, 0, 0, 1]]), axis=0)

    # transform 机器人->板
    # 以第一个点为基准
    mpoints_in_robot = np.array(rob_points_list)
    mpoints_in_self = np.ndarray([points_num, 1, 3], dtype=np.float32)

    for i in range(int((points_num / 4))):
        mpoints_in_self[i * 4 + 0, :, :] = np.array(
            [[0 * (side_length + seperation), 6 * side_length + 5 * seperation - i * (side_length + seperation), 0]])
        mpoints_in_self[i * 4 + 1, :, :] = np.array(
            [[1 * (side_length + seperation), 6 * side_length + 5 * seperation - i * (side_length + seperation), 0]])
        mpoints_in_self[i * 4 + 2, :, :] = np.array(
            [[2 * (side_length + seperation), 6 * side_length + 5 * seperation - i * (side_length + seperation), 0]])
        mpoints_in_self[i * 4 + 3, :, :] = np.array(
            [[3 * (side_length + seperation), 6 * side_length + 5 * seperation - i * (side_length + seperation), 0]])

    # 得到 机器人->板 的转换矩阵
    mpoints_robot_mtx = np.ndarray([points_num, 3], dtype=np.float32)
    mpoints_self_mtx = np.ndarray([points_num, 3], dtype=np.float32)
    for i in range(points_num):
        mpoints_robot_mtx[i] = mpoints_in_robot[0][i]
        mpoints_self_mtx[i] = mpoints_in_self[i]
    # mpoints_robot_mtx 和 mpoints_self_mtx中的点是按照顺序排列的
    Hr2m = get_pose_by_svd([mpoints_robot_mtx], [mpoints_self_mtx])[0]
    # print(np.dot(Hr2m, np.append(mpoints_robot_mtx.T, np.ones([1, 24]), axis=0)).T)

    # validate the reprojection errors
    sum_depth = 0
    for index in range(points_num):
        test = np.append(rob_points_list[0][index], [[1]], axis=1)
        test = test.T
        # print(np.dot(Hrm, test))
        test_points_in_camera = np.dot(extrinsic, np.dot(Hr2m, test))
        p = np.dot(mtx, test_points_in_camera[:3, :])
        # p[2][0] is the depth value "Z"
        sum_depth += p[2, 0]
        p = p / p[2, 0]
        # print("calculated:" + str(p[:2, 0].T) + "--" + "shot:" + str(corners[index][0][0]))
        cv2.circle(img_data, (int(p[:2, 0].T[0]), int(p[:2, 0].T[1])), 1, (0, 0, 255), -1)
        cv2.circle(img_data, (int(b[index, 0, 0]), int(b[index, 0, 1])), 1, (0, 0, 255), -1)

    depth = sum_depth / points_num
    # 画出重投影对比图
    cv2.namedWindow("1", cv2.WINDOW_NORMAL)
    cv2.imshow("1", img_data)
    cv2.waitKey(1000)

    # 得到 机器人->相机 的转换矩阵
    Hr2c = np.dot(extrinsic, Hr2m)
    # 求逆，得到 相机->机器人 的转换矩阵
    Hc2r = np.linalg.inv(Hr2c)
    return Hc2r, depth
    # given_pixel_points.append(1)
    # given_pixel_points = np.array(given_pixel_points).reshape(3, 1)
    # goal_points_in_camera = depth * np.dot(np.linalg.inv(mtx), given_pixel_points)
    # goal_points_in_camera = np.append(goal_points_in_camera, [[1]], axis=0)
    # goal_points_in_marker = np.dot(np.linalg.inv(extrinsic), goal_points_in_camera)
    # goal_points_in_robot = np.dot(np.linalg.inv(Hrm), goal_points_in_marker)
    # return goal_points_in_robot.reshape(1, -1)[0, 0:3]

def seq_img_update(side_length, seperation, marker_X, marker_Y, img_abspath):
    '''
    :param side_length: marker板tag的边长
    :param seperation: marker板tag之间的间距
    :param marker_X: 行方向上的标记个数
    :param marker_Y: 列方向上的标记个数
    :param img_abspath: 图片的绝对路径
    :return: 带标号的图片
    '''

    # 2590 * 1942
    img_data = cv2.imread(img_abspath, -1)
    img_data = cv2.undistort(img_data, mtx, dist)
    if len(img_data.shape) == 3:
        # 通道为3，则转为灰度图
        img_data = cv2.cvtColor(img_data, cv2.COLOR_BGR2GRAY)
    else:
        # 通道已经为1，则不变
        pass
    dictionary = aruco.Dictionary_get(aruco.DICT_6X6_100)
    board = aruco.GridBoard_create(marker_X, marker_Y, side_length, seperation, dictionary, 0)
    parameters = aruco.DetectorParameters_create()
    # corners是List， ids是narray
    corners, ids, rejectedImgPoints = aruco.detectMarkers(img_data, dictionary, parameters=parameters)

    assert len(ids) == points_num, "tag没有完全识别！请重新拍照"

    # b是角点的像素坐标
    b = np.ndarray([points_num, 1, 2], dtype=np.float32)

    if len(corners) == points_num:
        for i in range(points_num):
            # 基准点在机器人底座坐标系下的坐标
            # 以传送带移动方向为正方向，沿其反方向延伸，所以是-L
            b[i, 0, :] = corners[i][0][0, :]
            cv2.putText(img_data, str(int(ids[i][0])), (int(b[i, 0, 0]), int(b[i, 0, 1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)
            cv2.circle(img_data, (int(b[i, 0, 0]), int(b[i, 0, 1])), 5, (0, 0, 255), -1)

        cv2.namedWindow("corner_points", cv2.WINDOW_NORMAL)
        cv2.imshow("corner_points", img_data)
        cv2.waitKey(1000)
    return img_data